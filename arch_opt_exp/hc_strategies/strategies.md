# Overview of Strategies for Dealing with Hidden Constraints

Hidden constraints are constraints expressed through deterministic evaluation failures. They are hidden in the sense
that there is no analytical or mathematical description available of these constraints, but rather are discovered
on-the-fly when evaluating design points. They are deterministic in the sense that repeatedly evaluating the same point
will repeatedly lead to failure (i.e. the failure is not occurring randomly).
In the taxonomy of [LeDigabel2015], hidden constraints are classified as NUSH:
- **Nonquantifiable**: when the constraint is satisfied or violated, it is not known by "how much"
- **Unrelaxable**: only when the constraint is satisfied, the results are meaningful to the optimizer
- **Simulation**: the status of the constraint can only be found by running the (costly) simulation
- **Hidden**: the constraint is not explicitly defined in the problem formulation

Hidden constraints are also known as: unknown constraints, unspecified constraints, forgotten constraints, failures,
missing data, failure regions, non-computable domains/constraints, nonevaluable points, virtual constraints,
crash constraints.

In this work, hidden constraints are considered in the context of global Surrogate-Based Optimization (SBO).
SBO algorithms work by fitting some surrogate model to a dataset (generated by a Design of Experiments), and then
searching the surrogate model for an interesting infill point using an acquisition function. The infill point is then
evaluated and added to the dataset, after which the fitting and infill search starts again.

Nomenclature:
- `x`: design vectors (`n` x `nx`)
- `f`: objective values, assumed to be minimized (`n` x `nf`)
- `g`: constraint values of explicit constraints, assumed satisfied if `g <= 0` (`n` x `ng`)
- failed point: x where the evaluation failed and therefore the hidden constraint is violated
- `x_valid`: valid (non-failed) subset of x (`n_valid` x `nx`); with corresponding `f_valid` and `g_valid`
- `x_failed`: subset of x where evaluation failed (`n_failed` x `nx`)

Known approaches for dealing with hidden constraints (for global optimization):
- Rejection [Mueller2019]: remove (reject) failed points from the population after evaluation
- Extreme barrier [Mueller2019]: replace `f` and `g` of failed points with `inf` (n/a for SBO)
- Neighbour values [Huyer2008]: replace `f` and `g` of failed points with valid values of neighbour points
- Trained replacement [Forrester2006]: replace `f` and `g` of failed points with upper bound (mean + variance) of a GP
  trained on only the valid points
- Prediction [Lee2011]: split in `x_valid` and `x_failed`, use `x_failed` to train some model to predict regions with
  hidden constraints (at some confidence level), use some evasion strategy to stay away from predicted hidden
  constraint violations

Prediction models and evasion strategies used (in an SBO context):
- Random forest classifier; multiply IE by probability of a valid response (Expected Feasible Improvement EFI) [Lee2011]
- Piecewise linear approximation using RBF; dynamically adjusted threshold (more strict as the maximum nr of evaluations
  is approached, see Eq. 7) for rejecting infill points [Mueller2019]
- Support Vector Machine (SVM) classifier with RBF kernel; Probability of Feasibility (PoF) constraint [Alimo2018]
- Gaussian Process Classifier (GPC); constraint-weighted acquisition function (EFI) [Snoek2013,Gelbart2014]
- Gaussian Process Classifier (GPC) conditioned on signs (-1 or 1) with mean at 0 (not failed); EFI [Bachoc2020]
- Least Squares SVM; EFI or PoF constraint [Sacher2018] (they show that using a probability threshold to 0.5 works best)

References:
- [Alimo2018](https://link.springer.com/chapter/10.1007/978-3-319-89890-2_17):
  Alimo, Shahrouz Ryan, Pooriya Beyhaghi, and Thomas R. Bewley. "Delaunay-based global optimization in nonconvex domains defined by hidden constraints." In Evolutionary and Deterministic Methods for Design Optimization and Control With Applications to Industrial and Societal Problems, pp. 261-271. Cham: Springer International Publishing, 2018.
- [Forrester2006](https://royalsocietypublishing.org/doi/full/10.1098/rspa.2005.1608):
  Forrester, Alexander IJ, András Sóbester, and Andy J. Keane. "Optimization with missing data." Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 462, no. 2067 (2006): 935-945.
- [Bachoc2020](https://link.springer.com/article/10.1007/s10898-020-00920-0):
  Bachoc, François, Céline Helbert, and Victor Picheny. "Gaussian process optimization with failures: classification and convergence proof." Journal of Global Optimization 78 (2020): 483-506.
- [Gelbart2014](https://arxiv.org/abs/1403.5607):
  Gelbart, Michael A., Jasper Snoek, and Ryan P. Adams. "Bayesian optimization with unknown constraints." arXiv preprint arXiv:1403.5607 (2014).
- [Huyer2008](https://dl.acm.org/doi/abs/10.1145/1377612.1377613):
  Huyer, Waltraud, and Arnold Neumaier. "SNOBFIT--stable noisy optimization by branch and fit." ACM Transactions on Mathematical Software (TOMS) 35, no. 2 (2008): 1-25.
- [LeDigabel2015](https://arxiv.org/abs/1505.07881):
  Digabel, Sébastien Le, and Stefan M. Wild. "A taxonomy of constraints in simulation-based optimization." arXiv preprint arXiv:1505.07881 (2015).
- [Lee2011](https://tr.soe.ucsc.edu/sites/default/files/technical-reports/UCSC-SOE-10-10.pdf):
  Lee, H., R. Gramacy, Crystal Linkletter, and G. Gray. "Optimization subject to hidden constraints via statistical emulation." Pacific Journal of Optimization 7, no. 3 (2011): 467-478.
- [Mueller2019](https://pubsonline.informs.org/doi/abs/10.1287/ijoc.2018.0864):
  Juliane Müller, Marcus Day (2019) Surrogate Optimization of Computationally Expensive Black-Box Problems with Hidden Constraints. INFORMS Journal on Computing 31(4):689-702.
- [Sacher2018](https://link.springer.com/article/10.1007/s00158-018-1981-8):
  Sacher, Matthieu, Régis Duvigneau, Olivier Le Maitre, Mathieu Durand, Elisa Berrini, Frédéric Hauville, and Jacques-André Astolfi. "A classification approach to efficient global optimization in presence of non-computable domains." Structural and Multidisciplinary Optimization 58 (2018): 1537-1557.
- [Snoek2013](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=2e079604c7a00c43f06e214280cea18a89dcecef):
  Snoek, Jasper Roland. "Bayesian optimization and semiparametric models with applications to assistive technology." PhD diss., University of Toronto, 2013.

## Experiments

Hidden constraint strategies can be broken down as follows:
1. Rejection
2. Value replacement
   1. *Global worst*: replace failed outputs by the worst values in the valid dataset
   2. *Local worst*: replace failed outputs by values of the closest valid points
   3. *Neighborhood worst*: replace failed outputs by the worst values of the closest `n` valid points
   4. *Neighborhood mean*: replace failed outputs by the mean of the values of the closest `n` valid points
   5. *Predicted worst*: replace failed outputs by the (mean + var) of a GP trained on the valid dataset
3. Prediction
   1. Prediction model choice (see also: https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)
      1. Random forest classifier
      2. k-nearest neighbors classifier
      3. Gaussian process classifier
      4. Least squares support vector machine
      5. RBF support vector machine
      6. Linear RBF regressor
      7. Gaussian process regressor
      8. Variational gaussian processes (variational Bayesian gaussian mixture)
   2. Evasion strategy
      1. Acquisition objective penalty
      2. Acquisition constraint

Note: we do not consider a variable confidence level threshold for the constraint evasion strategy (as used in
[Mueller2019]), because we want the SBO algorithm to be stateless.
